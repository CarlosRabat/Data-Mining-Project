{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from river.drift import ADWIN\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, is_leaf=True, prediction=None):\n",
    "        self.is_leaf = is_leaf\n",
    "        self.prediction = prediction\n",
    "        self.split_feature = None\n",
    "        self.split_value = None\n",
    "        self.children = {}\n",
    "        self.class_counts = np.zeros(2)\n",
    "        self.adwin = ADWIN()\n",
    "        self.alternate_tree = None  # Alternate tree starts as None\n",
    "\n",
    "def hoeffding_bound(R, n):\n",
    "    return np.sqrt((R**2 * np.log(1/0.10)) / (2 * n))\n",
    "\n",
    "def entropy(labels):\n",
    "    label_counts = np.bincount(labels, minlength=2)\n",
    "    probabilities = label_counts / np.sum(label_counts)\n",
    "    probabilities = probabilities[probabilities > 0]\n",
    "    return -np.sum(probabilities * np.log2(probabilities))\n",
    "\n",
    "def information_gain(parent_labels, left_labels, right_labels):\n",
    "    entropy_before = entropy(parent_labels)\n",
    "    total_size = len(parent_labels)\n",
    "    left_size = len(left_labels)\n",
    "    right_size = len(right_labels)\n",
    "    weighted_entropy = (left_size / total_size) * entropy(left_labels) + \\\n",
    "                        (right_size / total_size) * entropy(right_labels)\n",
    "    return entropy_before - weighted_entropy\n",
    "\n",
    "def best_split(data, labels):\n",
    "    features = data.shape[1]\n",
    "    best_split_feature = None\n",
    "    best_split_value = None\n",
    "    best_split_information_gain = -np.inf\n",
    "    \n",
    "    for feature in range(features):\n",
    "        values = np.sort(np.unique(data[:, feature]))\n",
    "        for i in range(len(values) - 1):\n",
    "            split_value = (values[i] + values[i+1]) / 2\n",
    "            smaller_values = data[:, feature] <= split_value\n",
    "            bigger_values = ~smaller_values\n",
    "            info_gain = information_gain(labels, labels[smaller_values], labels[bigger_values])\n",
    "            if info_gain > best_split_information_gain:\n",
    "                best_split_feature = feature\n",
    "                best_split_value = split_value\n",
    "                best_split_information_gain = info_gain\n",
    "                \n",
    "    return best_split_feature, best_split_value\n",
    "\n",
    "class HAT:\n",
    "    def __init__(self):\n",
    "        self.root = Node(is_leaf=True, prediction=0)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        y_adjusted = y - 1\n",
    "        for xi, yi in zip(X, y_adjusted):\n",
    "            self._fit_single(xi, yi)\n",
    "    \n",
    "    def _fit_single(self, x, y):\n",
    "        node = self.root\n",
    "        while not node.is_leaf:\n",
    "            if x[node.split_feature] <= node.split_value:\n",
    "                node = node.children['left']\n",
    "            else:\n",
    "                node = node.children['right']\n",
    "        \n",
    "        node.class_counts[y] += 1\n",
    "        node.prediction = np.argmax(node.class_counts)\n",
    "        \n",
    "        # ADWIN update and drift check\n",
    "        old_prediction = node.prediction\n",
    "        node.adwin.update(y == old_prediction)\n",
    "        \n",
    "        if node.adwin.drift_detected:\n",
    "            if node.alternate_tree is None:\n",
    "                node.alternate_tree = Node(is_leaf=True, prediction=np.argmax(node.class_counts))\n",
    "            self._fit_single(x, y, node.alternate_tree)  # Train alternate tree\n",
    "        \n",
    "        # Compare performance if alternate tree exists\n",
    "        if node.alternate_tree and node.alternate_tree.adwin.estimation < node.adwin.estimation:\n",
    "            # Replace subtree\n",
    "            node.is_leaf = node.alternate_tree.is_leaf\n",
    "            node.split_feature = node.alternate_tree.split_feature\n",
    "            node.split_value = node.alternate_tree.split_value\n",
    "            node.children = node.alternate_tree.children\n",
    "        \n",
    "        # Attempt to split if the node is still a leaf\n",
    "        if node.is_leaf and np.sum(node.class_counts) > 25:\n",
    "            self._attempt_to_split(node, x, y)\n",
    "    \n",
    "    def _attempt_to_split(self, node, x, y):\n",
    "        X_sub = np.array([x])\n",
    "        y_sub = np.array([y])\n",
    "        feature, value = best_split(X_sub, y_sub)\n",
    "        if feature is not None:\n",
    "            n = np.sum(node.class_counts)\n",
    "            epsilon = hoeffding_bound(1, n)\n",
    "            if epsilon < 0.1: \n",
    "                node.is_leaf = False\n",
    "                node.split_feature = feature\n",
    "                node.split_value = value\n",
    "                node.children['left'] = Node(is_leaf=True, prediction=np.argmax(node.class_counts))\n",
    "                node.children['right'] = Node(is_leaf=True, prediction=np.argmax(node.class_counts))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        node = self.root\n",
    "        while not node.is_leaf:\n",
    "            if x[node.split_feature] <= node.split_value:\n",
    "                node = node.children['left']\n",
    "            else:\n",
    "                node = node.children['right']\n",
    "        return node.prediction + 1  # Adjust back to original class labels\n",
    "\n",
    "# You would then use the HAT class with your data similarly to how you'd use the EFDT.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
