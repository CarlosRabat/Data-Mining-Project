{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from river.drift import ADWIN\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, is_leaf=True, prediction=None):\n",
    "        self.is_leaf = is_leaf\n",
    "        self.prediction = prediction\n",
    "        self.split_feature = None\n",
    "        self.split_value = None\n",
    "        self.children = {}\n",
    "        self.class_counts = np.zeros(2)\n",
    "        self.adwin = ADWIN()\n",
    "        self.alternate_tree = None  # Alternate tree starts as None\n",
    "\n",
    "def hoeffding_bound(R, n):\n",
    "    return np.sqrt((R**2 * np.log(1/0.10)) / (2 * n))\n",
    "\n",
    "def entropy(labels):\n",
    "    label_counts = np.bincount(labels, minlength=2)\n",
    "    probabilities = label_counts / np.sum(label_counts)\n",
    "    probabilities = probabilities[probabilities > 0]\n",
    "    return -np.sum(probabilities * np.log2(probabilities))\n",
    "\n",
    "def information_gain(parent_labels, left_labels, right_labels):\n",
    "    entropy_before = entropy(parent_labels)\n",
    "    total_size = len(parent_labels)\n",
    "    left_size = len(left_labels)\n",
    "    right_size = len(right_labels)\n",
    "    weighted_entropy = (left_size / total_size) * entropy(left_labels) + \\\n",
    "                        (right_size / total_size) * entropy(right_labels)\n",
    "    return entropy_before - weighted_entropy\n",
    "\n",
    "def best_split(data, labels):\n",
    "    features = data.shape[1]\n",
    "    best_split_feature = None\n",
    "    best_split_value = None\n",
    "    best_split_information_gain = -np.inf\n",
    "    \n",
    "    for feature in range(features):\n",
    "        values = np.sort(np.unique(data[:, feature]))\n",
    "        for i in range(len(values) - 1):\n",
    "            split_value = (values[i] + values[i+1]) / 2\n",
    "            smaller_values = data[:, feature] <= split_value\n",
    "            bigger_values = ~smaller_values\n",
    "            info_gain = information_gain(labels, labels[smaller_values], labels[bigger_values])\n",
    "            if info_gain > best_split_information_gain:\n",
    "                best_split_feature = feature\n",
    "                best_split_value = split_value\n",
    "                best_split_information_gain = info_gain\n",
    "                \n",
    "    return best_split_feature, best_split_value\n",
    "\n",
    "class HAT:\n",
    "    def __init__(self):\n",
    "        self.root = Node(is_leaf=True, prediction=0)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        y_adjusted = y - 1\n",
    "        for xi, yi in zip(X, y_adjusted):\n",
    "            self._fit_single(xi, yi)\n",
    "    \n",
    "    def _fit_single(self, x, y):\n",
    "        y = y - 1\n",
    "        node = self.root\n",
    "        while not node.is_leaf:\n",
    "            if x[node.split_feature] <= node.split_value:\n",
    "                node = node.children['left']\n",
    "            else:\n",
    "                node = node.children['right']\n",
    "        \n",
    "        node.class_counts[y] += 1\n",
    "        node.prediction = np.argmax(node.class_counts)\n",
    "        \n",
    "        # ADWIN update and drift check\n",
    "        old_prediction = node.prediction\n",
    "        node.adwin.update(y == old_prediction)\n",
    "        \n",
    "        if node.adwin.drift_detected:\n",
    "            if node.alternate_tree is None:\n",
    "                node.alternate_tree = Node(is_leaf=True, prediction=np.argmax(node.class_counts))\n",
    "            self._fit_single(x, y, node.alternate_tree)  # Train alternate tree\n",
    "        \n",
    "        # Compare performance if alternate tree exists\n",
    "        if node.alternate_tree and node.alternate_tree.adwin.estimation < node.adwin.estimation:\n",
    "            # Replace subtree\n",
    "            node.is_leaf = node.alternate_tree.is_leaf\n",
    "            node.split_feature = node.alternate_tree.split_feature\n",
    "            node.split_value = node.alternate_tree.split_value\n",
    "            node.children = node.alternate_tree.children\n",
    "        \n",
    "        # Attempt to split if the node is still a leaf\n",
    "        if node.is_leaf and np.sum(node.class_counts) > 25:\n",
    "            self._attempt_to_split(node, x, y)\n",
    "    \n",
    "    def _attempt_to_split(self, node, x, y):\n",
    "        X_sub = np.array([x])\n",
    "        y_sub = np.array([y])\n",
    "        feature, value = best_split(X_sub, y_sub)\n",
    "        if feature is not None:\n",
    "            n = np.sum(node.class_counts)\n",
    "            epsilon = hoeffding_bound(1, n)\n",
    "            if epsilon < 0.1: \n",
    "                node.is_leaf = False\n",
    "                node.split_feature = feature\n",
    "                node.split_value = value\n",
    "                node.children['left'] = Node(is_leaf=True, prediction=np.argmax(node.class_counts))\n",
    "                node.children['right'] = Node(is_leaf=True, prediction=np.argmax(node.class_counts))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        node = self.root\n",
    "        while not node.is_leaf:\n",
    "            if x[node.split_feature] <= node.split_value:\n",
    "                node = node.children['left']\n",
    "            else:\n",
    "                node = node.children['right']\n",
    "        return node.prediction + 1  # Adjust back to original class labels\n",
    "\n",
    "# You would then use the HAT class with your data similarly to how you'd use the EFDT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming your file is named 'data.txt' and located in the current directory\n",
    "file_path = 'Skin_NonSkin 2.txt'\n",
    "\n",
    "# Load the data\n",
    "data = np.loadtxt(file_path, delimiter='\\t')\n",
    "np.random.shuffle(data)\n",
    "\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1].astype(int) \n",
    "\n",
    "current = 0\n",
    "drift = False\n",
    "\n",
    "for i in range(len(y)):\n",
    "    if drift:\n",
    "        y[i] = 1 if y[i] == 2 else 2\n",
    "    current += 1\n",
    "    \n",
    "    # Changes every 50000\n",
    "    if current > 50000:\n",
    "        drift = not drift\n",
    "        current = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance: 0\n",
      "Instance: 10000\n",
      "Instance: 20000\n",
      "Instance: 30000\n",
      "Instance: 40000\n",
      "Instance: 50000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "HAT._fit_single() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInstance: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 81\u001b[0m, in \u001b[0;36mHAT._fit_single\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39malternate_tree \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m         node\u001b[38;5;241m.\u001b[39malternate_tree \u001b[38;5;241m=\u001b[39m Node(is_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, prediction\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39margmax(node\u001b[38;5;241m.\u001b[39mclass_counts))\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malternate_tree\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Train alternate tree\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Compare performance if alternate tree exists\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39malternate_tree \u001b[38;5;129;01mand\u001b[39;00m node\u001b[38;5;241m.\u001b[39malternate_tree\u001b[38;5;241m.\u001b[39madwin\u001b[38;5;241m.\u001b[39mestimation \u001b[38;5;241m<\u001b[39m node\u001b[38;5;241m.\u001b[39madwin\u001b[38;5;241m.\u001b[39mestimation:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# Replace subtree\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: HAT._fit_single() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the Tree\n",
    "model = HAT()\n",
    "\n",
    "#Initialize variables\n",
    "errors_count = 0\n",
    "error_rates = []\n",
    "\n",
    "for idx in range(len(y)):\n",
    "    pred = model.predict(X[idx])\n",
    "\n",
    "    if pred != y[idx]:\n",
    "        errors_count += 1\n",
    "        \n",
    "    \n",
    "    # Calculate Error Rate\n",
    "    \n",
    "    if idx > 10000:\n",
    "        error_rate = errors_count / (idx + 1)\n",
    "        error_rates.append([idx, error_rate])\n",
    "    \n",
    "    # Print Every 10000 Iterations\n",
    "    if idx % 10000 == 0:\n",
    "        print(f'Instance: {idx}')\n",
    "    \n",
    "\n",
    "    model._fit_single(X[idx], y[idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Splitting the list into two lists, idxs and errors\n",
    "idxs, errors = zip(*error_rates)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))  # Optional: Specifies the figure size\n",
    "plt.plot(idxs, errors, marker='o', linestyle='-', color='b')  # Marker, linestyle, and color are optional\n",
    "plt.title('Error Rate Over Time')\n",
    "plt.xlabel('Index (or Time)')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.grid(True)  # Optional: Adds a grid for easier reading\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
